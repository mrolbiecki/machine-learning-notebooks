{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f148e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087929fd",
   "metadata": {},
   "source": [
    "This notebook is merged from multiple notebooks created while working on this task. Each numbered section corresponds to one of the original notebooks.\n",
    "\n",
    "# Classification and Regression of Artificial Data\n",
    "\n",
    "This project involves working with an artificially generated dataset consisting of 2000 samples, 400 input variables, and two output variables: a discrete 'Class' variable (0 or 1) and a continuous 'Output' variable. The goal is to build ML models predicting these output variables based on the input features and analyze the dependencies discovered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e294f88",
   "metadata": {},
   "source": [
    "## 0. Testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffebc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "def evaluate_on_validation_data(validation_file_path):\n",
    "    \"\"\"Process a validation_data.csv file with the same structure as the training data\n",
    "    and compute the classification accuracies and R^2 values for both baseline and best models.\n",
    "    \n",
    "    Args:\n",
    "        validation_file_path: Path to the validation CSV file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with all computed metrics\n",
    "    \"\"\"\n",
    "    # Load validation data\n",
    "    validation_df = pd.read_csv(validation_file_path, sep=';')\n",
    "    X_val = validation_df.drop(['Class', 'Output'], axis=1)\n",
    "    y_val_class = validation_df['Class']\n",
    "    y_val_output = validation_df['Output']\n",
    "    \n",
    "    # Load and prepare training data (needed to fit models)\n",
    "    train_df = pd.read_csv('data.csv', sep=';')\n",
    "    X_train = train_df.drop(['Class', 'Output'], axis=1)\n",
    "    y_train_class = train_df['Class']\n",
    "    y_train_output = train_df['Output']\n",
    "    \n",
    "    # Classification baseline model (Logistic Regression)\n",
    "    baseline_clf = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')\n",
    "    baseline_clf.fit(X_train, y_train_class)\n",
    "    baseline_clf_accuracy = accuracy_score(y_val_class, baseline_clf.predict(X_val))\n",
    "    \n",
    "    # Best classification model (Random Forest with RFE)\n",
    "    # Create and fit RFE\n",
    "    best_feature_count = 16  # From our optimization\n",
    "    optimal_rfe = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), \n",
    "                      n_features_to_select=best_feature_count, step=10)\n",
    "    optimal_rfe.fit(X_train, y_train_class)\n",
    "    X_train_rfe = optimal_rfe.transform(X_train)\n",
    "    X_val_rfe = optimal_rfe.transform(X_val)\n",
    "    \n",
    "    # Best RF parameters from grid search\n",
    "    best_rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=30,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        bootstrap=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    best_rf.fit(X_train_rfe, y_train_class)\n",
    "    best_clf_accuracy = accuracy_score(y_val_class, best_rf.predict(X_val_rfe))\n",
    "    \n",
    "    # Regression baseline model (Linear Regression)\n",
    "    baseline_reg = LinearRegression()\n",
    "    baseline_reg.fit(X_train, y_train_output)\n",
    "    baseline_reg_r2 = r2_score(y_val_output, baseline_reg.predict(X_val))\n",
    "    \n",
    "    # Best regression model (Lasso with selected features)\n",
    "    # Use Lasso to select features\n",
    "    feature_selector = Lasso(alpha=0.1, max_iter=10000)\n",
    "    feature_selector.fit(X_train, y_train_output)\n",
    "    \n",
    "    # Get non-zero coefficients\n",
    "    selected_features = X_train.columns[feature_selector.coef_ != 0].tolist()\n",
    "    X_train_lasso = X_train[selected_features]\n",
    "    X_val_lasso = X_val[selected_features]\n",
    "    \n",
    "    # Best Lasso parameters from tuning\n",
    "    best_lasso = Lasso(alpha=0.005, max_iter=10000)\n",
    "    best_lasso.fit(X_train_lasso, y_train_output)\n",
    "    best_reg_r2 = r2_score(y_val_output, best_lasso.predict(X_val_lasso))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Baseline Classification Accuracy: {baseline_clf_accuracy:.4f} ({baseline_clf_accuracy*100:.2f}%)\")\n",
    "    print(f\"Best Classification Accuracy:     {best_clf_accuracy:.4f} ({best_clf_accuracy*100:.2f}%)\")\n",
    "    print(f\"Baseline Regression R²:          {baseline_reg_r2:.4f}\")\n",
    "    print(f\"Best Regression R²:              {best_reg_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'baseline_classification_accuracy': baseline_clf_accuracy,\n",
    "        'best_classification_accuracy': best_clf_accuracy,\n",
    "        'baseline_regression_r2': baseline_reg_r2,\n",
    "        'best_regression_r2': best_reg_r2\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# results = evaluate_on_validation_data('validation_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce3b9b",
   "metadata": {},
   "source": [
    "## 1. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df0f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data exploration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7032db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Output</th>\n",
       "      <th>Input1</th>\n",
       "      <th>Input2</th>\n",
       "      <th>Input3</th>\n",
       "      <th>Input4</th>\n",
       "      <th>Input5</th>\n",
       "      <th>Input6</th>\n",
       "      <th>Input7</th>\n",
       "      <th>Input8</th>\n",
       "      <th>...</th>\n",
       "      <th>Input391</th>\n",
       "      <th>Input392</th>\n",
       "      <th>Input393</th>\n",
       "      <th>Input394</th>\n",
       "      <th>Input395</th>\n",
       "      <th>Input396</th>\n",
       "      <th>Input397</th>\n",
       "      <th>Input398</th>\n",
       "      <th>Input399</th>\n",
       "      <th>Input400</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.800586</td>\n",
       "      <td>-0.002583</td>\n",
       "      <td>2.184037</td>\n",
       "      <td>-0.322008</td>\n",
       "      <td>1.621241</td>\n",
       "      <td>1.192444</td>\n",
       "      <td>-0.278356</td>\n",
       "      <td>-0.207366</td>\n",
       "      <td>0.735689</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.140861</td>\n",
       "      <td>1.187660</td>\n",
       "      <td>0.345238</td>\n",
       "      <td>-0.844885</td>\n",
       "      <td>0.580007</td>\n",
       "      <td>-2.605781</td>\n",
       "      <td>-0.299471</td>\n",
       "      <td>0.711487</td>\n",
       "      <td>-0.753316</td>\n",
       "      <td>0.728763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.168475</td>\n",
       "      <td>0.668637</td>\n",
       "      <td>1.373933</td>\n",
       "      <td>-0.476868</td>\n",
       "      <td>-0.724704</td>\n",
       "      <td>0.031162</td>\n",
       "      <td>-1.845921</td>\n",
       "      <td>0.784890</td>\n",
       "      <td>1.508526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.286120</td>\n",
       "      <td>-0.900044</td>\n",
       "      <td>-0.500399</td>\n",
       "      <td>-0.126421</td>\n",
       "      <td>-0.632233</td>\n",
       "      <td>-2.557419</td>\n",
       "      <td>0.056044</td>\n",
       "      <td>0.634774</td>\n",
       "      <td>-0.259835</td>\n",
       "      <td>0.106390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.210777</td>\n",
       "      <td>-0.681438</td>\n",
       "      <td>-0.544753</td>\n",
       "      <td>0.441346</td>\n",
       "      <td>-0.019906</td>\n",
       "      <td>-0.192135</td>\n",
       "      <td>-0.162510</td>\n",
       "      <td>-0.998777</td>\n",
       "      <td>0.686472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391605</td>\n",
       "      <td>-0.190147</td>\n",
       "      <td>0.793746</td>\n",
       "      <td>-0.812737</td>\n",
       "      <td>-0.068228</td>\n",
       "      <td>-0.313143</td>\n",
       "      <td>2.564096</td>\n",
       "      <td>0.848355</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>-1.525615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.505678</td>\n",
       "      <td>-0.497957</td>\n",
       "      <td>0.720712</td>\n",
       "      <td>0.149120</td>\n",
       "      <td>0.019251</td>\n",
       "      <td>1.377850</td>\n",
       "      <td>0.981337</td>\n",
       "      <td>-0.846813</td>\n",
       "      <td>0.036790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176734</td>\n",
       "      <td>-0.947351</td>\n",
       "      <td>-0.888601</td>\n",
       "      <td>1.509450</td>\n",
       "      <td>-0.501929</td>\n",
       "      <td>-0.554909</td>\n",
       "      <td>-0.104051</td>\n",
       "      <td>0.442150</td>\n",
       "      <td>-0.056644</td>\n",
       "      <td>1.447267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-10.281033</td>\n",
       "      <td>-1.178544</td>\n",
       "      <td>0.176941</td>\n",
       "      <td>1.112202</td>\n",
       "      <td>1.234189</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>-0.773329</td>\n",
       "      <td>-0.811075</td>\n",
       "      <td>1.550537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181325</td>\n",
       "      <td>0.198960</td>\n",
       "      <td>-0.697497</td>\n",
       "      <td>-0.836371</td>\n",
       "      <td>1.652071</td>\n",
       "      <td>0.974292</td>\n",
       "      <td>1.584071</td>\n",
       "      <td>-0.202352</td>\n",
       "      <td>1.362426</td>\n",
       "      <td>1.023857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class     Output    Input1    Input2    Input3    Input4    Input5  \\\n",
       "0      0   0.800586 -0.002583  2.184037 -0.322008  1.621241  1.192444   \n",
       "1      0   2.168475  0.668637  1.373933 -0.476868 -0.724704  0.031162   \n",
       "2      1  -1.210777 -0.681438 -0.544753  0.441346 -0.019906 -0.192135   \n",
       "3      1   0.505678 -0.497957  0.720712  0.149120  0.019251  1.377850   \n",
       "4      1 -10.281033 -1.178544  0.176941  1.112202  1.234189  0.999451   \n",
       "\n",
       "     Input6    Input7    Input8  ...  Input391  Input392  Input393  Input394  \\\n",
       "0 -0.278356 -0.207366  0.735689  ... -2.140861  1.187660  0.345238 -0.844885   \n",
       "1 -1.845921  0.784890  1.508526  ... -1.286120 -0.900044 -0.500399 -0.126421   \n",
       "2 -0.162510 -0.998777  0.686472  ... -0.391605 -0.190147  0.793746 -0.812737   \n",
       "3  0.981337 -0.846813  0.036790  ... -0.176734 -0.947351 -0.888601  1.509450   \n",
       "4 -0.773329 -0.811075  1.550537  ... -0.181325  0.198960 -0.697497 -0.836371   \n",
       "\n",
       "   Input395  Input396  Input397  Input398  Input399  Input400  \n",
       "0  0.580007 -2.605781 -0.299471  0.711487 -0.753316  0.728763  \n",
       "1 -0.632233 -2.557419  0.056044  0.634774 -0.259835  0.106390  \n",
       "2 -0.068228 -0.313143  2.564096  0.848355  0.180556 -1.525615  \n",
       "3 -0.501929 -0.554909 -0.104051  0.442150 -0.056644  1.447267  \n",
       "4  1.652071  0.974292  1.584071 -0.202352  1.362426  1.023857  \n",
       "\n",
       "[5 rows x 402 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data.csv', sep=';')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aece096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2000, 402)\n",
      "\n",
      "Class distribution:\n",
      "Class\n",
      "1    1013\n",
      "0     987\n",
      "Name: count, dtype: int64\n",
      "Class 0: 49.35%\n",
      "Class 1: 50.65%\n",
      "\n",
      "Output variable statistics:\n",
      "count    2000.000000\n",
      "mean        0.106647\n",
      "std         3.562855\n",
      "min       -12.384019\n",
      "25%        -2.310265\n",
      "50%         0.140658\n",
      "75%         2.435991\n",
      "max        11.676146\n",
      "Name: Output, dtype: float64\n",
      "\n",
      "Missing values:\n",
      "0\n",
      "\n",
      "Features shape: (2000, 400)\n",
      "Number of predictor variables: 400\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data exploration\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "class_counts = df['Class'].value_counts()\n",
    "print(class_counts)\n",
    "print(f\"Class 0: {class_counts[0]/len(df)*100:.2f}%\")\n",
    "print(f\"Class 1: {class_counts[1]/len(df)*100:.2f}%\")\n",
    "\n",
    "# Continuous output variable statistics\n",
    "print(\"\\nOutput variable statistics:\")\n",
    "print(df['Output'].describe())\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sum())\n",
    "\n",
    "# Separate features and targets\n",
    "X = df.drop(['Class', 'Output'], axis=1)  # Use all Input variables as predictors\n",
    "y_class = df['Class']  # Classification target\n",
    "y_output = df['Output']  # Regression target\n",
    "\n",
    "# Feature information\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Number of predictor variables: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560efbc5",
   "metadata": {},
   "source": [
    "## 2. Baseline Classification Model\n",
    "\n",
    "We'll implement a logistic regression model as our baseline for the classification task, using all 400 input variables. We'll evaluate its performance on the training data and use cross-validation to estimate its ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "671ca029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7100 (71.00%)\n"
     ]
    }
   ],
   "source": [
    "# Build Logistic Regression Model\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X, y_class)\n",
    "\n",
    "# Make predictions on training data\n",
    "y_train_pred = logistic_model.predict(X)\n",
    "y_train_pred_proba = logistic_model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = accuracy_score(y_class, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ed7139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "Training Accuracy:     0.7100 (71.00%)\n",
      "CV Mean Accuracy:      0.5170 (51.70%)\n",
      "Difference:            0.1930\n"
     ]
    }
   ],
   "source": [
    "# Use 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(logistic_model, X, y_class, cv=kf, scoring='accuracy')\n",
    "cv_scores_mean = cv_scores.mean()\n",
    "\n",
    "# Compare training vs cross-validation performance\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"Training Accuracy:     {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"CV Mean Accuracy:      {cv_scores_mean:.4f} ({cv_scores_mean*100:.2f}%)\")\n",
    "print(f\"Difference:            {train_accuracy - cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf0df8",
   "metadata": {},
   "source": [
    "### Baseline Classification Model Analysis\n",
    "\n",
    "The logistic regression model trained on all 400 features shows signs of overfitting. The large gap between training accuracy and cross-validation accuracy indicates the model is not generalizing well to unseen data. This suggests that many of the 400 features might be noise rather than signal for the classification task, and dimensionality reduction or feature selection could improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b7683",
   "metadata": {},
   "source": [
    "## 3. Advanced classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc061974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation framework for classification\n",
    "def evaluate_classification_model(model, X, y, cv_folds=5, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive model evaluation function for classification\"\"\"\n",
    "    # K-Fold for cross-validation\n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    \n",
    "    # Fit model for training accuracy\n",
    "    model.fit(X, y)\n",
    "    train_pred = model.predict(X)\n",
    "    train_accuracy = accuracy_score(y, train_pred)\n",
    "    \n",
    "    # Results\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'cv_scores': cv_scores,\n",
    "        'overfitting': train_accuracy - cv_scores.mean()\n",
    "    }\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "    print(f\"CV Mean Accuracy:  {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)\")\n",
    "    print(f\"Overfitting:       {train_accuracy - cv_scores.mean():.4f}\")\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191df44",
   "metadata": {},
   "source": [
    "### Model Evaluation Summary\n",
    "\n",
    "After exploring several approaches, here are the performance results of different models sorted by cross-validation accuracy:\n",
    "\n",
    "| Model | CV Accuracy | Improvement |\n",
    "|-------|------------|-------------|\n",
    "| Baseline Logistic Regression | 52.4% | Baseline |\n",
    "| Logistic Regression + SelectKBest | 61.1% | +8.7% |\n",
    "| SVC (RBF) + SelectKBest | 67.5% | +15.1% |\n",
    "| Random Forest | 66.1% | +13.7% |\n",
    "| Random Forest + SelectKBest | 73.6% | +21.2% |\n",
    "| Optimized Random Forest + SelectKBest | 74.9% | +22.5% |\n",
    "| Random Forest + PCA + Lasso | 65.3% | +12.9% |\n",
    "| **Random Forest + RFE** | **78.8%** | **+26.4%** |\n",
    "\n",
    "The most promising approach uses Random Forest with Recursive Feature Elimination (RFE). Let's optimize this model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44692f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing RFE with 10 features...\n",
      "Model: RF + RFE (10 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.7950 (79.50%)\n",
      "Overfitting:       0.2050\n",
      "\n",
      "Testing RFE with 12 features...\n",
      "Model: RF + RFE (10 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.7950 (79.50%)\n",
      "Overfitting:       0.2050\n",
      "\n",
      "Testing RFE with 12 features...\n",
      "Model: RF + RFE (12 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.8100 (81.00%)\n",
      "Overfitting:       0.1900\n",
      "\n",
      "Testing RFE with 14 features...\n",
      "Model: RF + RFE (12 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.8100 (81.00%)\n",
      "Overfitting:       0.1900\n",
      "\n",
      "Testing RFE with 14 features...\n",
      "Model: RF + RFE (14 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.8145 (81.45%)\n",
      "Overfitting:       0.1855\n",
      "\n",
      "Testing RFE with 16 features...\n",
      "Model: RF + RFE (14 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.8145 (81.45%)\n",
      "Overfitting:       0.1855\n",
      "\n",
      "Testing RFE with 16 features...\n",
      "Model: RF + RFE (16 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.8120 (81.20%)\n",
      "Overfitting:       0.1880\n",
      "\n",
      "Testing RFE with 18 features...\n",
      "Model: RF + RFE (16 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.8120 (81.20%)\n",
      "Overfitting:       0.1880\n",
      "\n",
      "Testing RFE with 18 features...\n",
      "Model: RF + RFE (18 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.8130 (81.30%)\n",
      "Overfitting:       0.1870\n",
      "\n",
      "Testing RFE with 20 features...\n",
      "Model: RF + RFE (18 features)\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "CV Mean Accuracy:  0.8130 (81.30%)\n",
      "Overfitting:       0.1870\n",
      "\n",
      "Testing RFE with 20 features...\n"
     ]
    }
   ],
   "source": [
    "# Find optimal number of features for RFE\n",
    "\n",
    "# Test a range of feature counts\n",
    "feature_range = [14, 16, 18, 20, 22, 24, 26]\n",
    "rfe_results = {}\n",
    "\n",
    "for n_features in feature_range:\n",
    "    print(f\"\\nTesting RFE with {n_features} features...\")\n",
    "    \n",
    "    # Create RFE with specified number of features\n",
    "    rfe_test = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), \n",
    "                  n_features_to_select=n_features, step=10, verbose=0)\n",
    "    X_rfe_test = rfe_test.fit_transform(X, y_class)\n",
    "    \n",
    "    # Create and evaluate model with default parameters\n",
    "    rf_test = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Use cross-validation for more reliable results\n",
    "    cv_results = evaluate_classification_model(rf_test, X_rfe_test, y_class, model_name=f\"RF + RFE ({n_features} features)\")\n",
    "    rfe_results[n_features] = cv_results\n",
    "\n",
    "# Find best feature count\n",
    "feature_counts = list(rfe_results.keys())\n",
    "cv_means = [rfe_results[n]['cv_mean'] for n in feature_counts]\n",
    "best_feature_count = feature_counts[np.argmax(cv_means)]\n",
    "best_cv_mean = max(cv_means)\n",
    "\n",
    "print(f\"\\nBest feature count: {best_feature_count} with CV accuracy: {best_cv_mean:.4f} ({best_cv_mean*100:.2f}%)\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(feature_counts, [score*100 for score in cv_means], marker='o', linestyle='-')\n",
    "plt.axvline(x=best_feature_count, color='r', linestyle='--', label=f'Best: {best_feature_count} features')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cross-Validation Accuracy (%)')\n",
    "plt.title('RFE Feature Count Optimization')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Use the best feature count for further optimization\n",
    "optimal_rfe = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), \n",
    "                  n_features_to_select=best_feature_count, step=10, verbose=0)\n",
    "X_rfe_optimal = optimal_rfe.fit_transform(X, y_class)\n",
    "\n",
    "# Get feature names\n",
    "optimal_feature_indices = np.where(optimal_rfe.ranking_ == 1)[0]\n",
    "optimal_feature_names = X.columns[optimal_feature_indices]\n",
    "print(f\"\\nOptimal features selected: {optimal_feature_names.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960aeac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization for Random Forest\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_rfe_optimal, y_class)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f} ({grid_search.best_score_*100:.2f}%)\")\n",
    "\n",
    "# Get best model\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b40d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of best model with rigorous cross-validation\n",
    "evaluate_classification_model(best_rf, X_rfe_optimal, y_class, cv_folds=10, model_name=\"Fully Optimized RF + RFE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2c138",
   "metadata": {},
   "source": [
    "### Advanced Classification Model Analysis\n",
    "\n",
    "Our optimized Random Forest classifier with Recursive Feature Elimination (RFE) achieved significant improvements over the baseline model:\n",
    "\n",
    "1. **Feature Selection**: We identified the optimal number of features (16) out of 400, dramatically reducing dimensionality while improving performance.\n",
    "\n",
    "2. **Model Selection**: Random Forest outperformed other classifiers for this task.\n",
    "\n",
    "3. **Hyperparameter Optimization**: Grid search identified the optimal parameters, further improving performance.\n",
    "\n",
    "**Performance Improvement**: The final model achieved approximately 80% accuracy in cross-validation, compared to 52% for the baseline model, representing a substantial 28 percentage point improvement.\n",
    "\n",
    "\n",
    "While there is still some overfitting (difference between training and CV accuracy), this is expected with Random Forest models, and the high cross-validation accuracy indicates good generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2a827",
   "metadata": {},
   "source": [
    "## 4. Baseline regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Linear Regression Model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "linear_model.fit(X, y_output)\n",
    "\n",
    "# Make predictions on training data\n",
    "y_pred_train = linear_model.predict(X)\n",
    "\n",
    "# Calculate training R^2 and RMSE\n",
    "train_mse = mean_squared_error(y_output, y_pred_train)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_output, y_pred_train)\n",
    "\n",
    "print(f\"Training R² Score: {train_r2:.4f}\")\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b08348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate cross-validation scores for different metrics\n",
    "cv_r2_scores = cross_val_score(linear_model, X, y_output, cv=kf, scoring='r2')\n",
    "cv_neg_mse_scores = cross_val_score(linear_model, X, y_output, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert negative scores to positive\n",
    "cv_mse_scores = -cv_neg_mse_scores\n",
    "cv_rmse_scores = np.sqrt(cv_mse_scores)\n",
    "\n",
    "# Calculate mean\n",
    "cv_r2_mean = cv_r2_scores.mean()\n",
    "cv_rmse_mean = cv_rmse_scores.mean()\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"Training R²: {train_r2:.4f}, Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Mean CV R²: {cv_r2_scores.mean():.4f}, Mean CV RMSE: {cv_rmse_scores.mean():.4f}\")\n",
    "print(f\"Difference in R²: {train_r2 - cv_r2_mean:.4f}, Difference in RMSE: {train_rmse - cv_rmse_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5b1c7",
   "metadata": {},
   "source": [
    "### Baseline Regression Model Analysis\n",
    "\n",
    "The linear regression model with all 400 features shows significant overfitting:\n",
    "\n",
    "1. **High Training R²**: The model fits the training data very well, with an R² score near 1.0.\n",
    "\n",
    "2. **Poor Cross-Validation R²**: The much lower cross-validation R² indicates the model doesn't generalize well.\n",
    "\n",
    "3. **Large RMSE Difference**: The gap between training and cross-validation RMSE further confirms poor generalization.\n",
    "\n",
    "This suggests that, similar to the classification task, many of the 400 features might be noise rather than signal for predicting the 'Output' variable. Feature selection or regularization could help improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8002ae36",
   "metadata": {},
   "source": [
    "## 5. Advanced regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061005b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function for regression models\n",
    "def evaluate_regression_model(model, X, y):\n",
    "    \"\"\"Evaluate regression model with training metrics and cross-validation\"\"\"\n",
    "    # 5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Get training metrics\n",
    "    model.fit(X, y)\n",
    "    y_pred_train = model.predict(X)\n",
    "    train_mse = mean_squared_error(y, y_pred_train)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    train_r2 = r2_score(y, y_pred_train)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "    cv_neg_mse_scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Convert negative scores to positive\n",
    "    cv_mse_scores = -cv_neg_mse_scores\n",
    "    cv_rmse_scores = np.sqrt(cv_mse_scores)\n",
    "    \n",
    "    # Calculate mean\n",
    "    cv_r2_mean = cv_r2_scores.mean()\n",
    "    cv_rmse_mean = cv_rmse_scores.mean()\n",
    "    \n",
    "    print(f\"Training R²: {train_r2:.4f}, Training RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Mean CV R²: {cv_r2_mean:.4f}, Mean CV RMSE: {cv_rmse_mean:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'train_r2': train_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'cv_r2': cv_r2_mean,\n",
    "        'cv_rmse': cv_rmse_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a33ab",
   "metadata": {},
   "source": [
    "### Model Evaluation Results\n",
    "\n",
    "After testing various regression models with different preprocessing techniques, here are the results sorted by cross-validation R² score (higher is better):\n",
    "\n",
    "| Model | CV R² | CV RMSE |\n",
    "|-------|-------|--------|\n",
    "| Lasso(alpha=0.1) | 0.4861 | 2.5445 |\n",
    "| LinearRegression_RFE(n=20) | 0.4859 | 2.5433 |\n",
    "| LinearRegression_RFE(n=30) | 0.4708 | 2.5799 |\n",
    "| LinearRegression_SelectKBest(k=30) | 0.4635 | 2.6003 |\n",
    "| LinearRegression_SelectKBest(k=50) | 0.4487 | 2.6335 |\n",
    "| LinearRegression_SelectKBest(k=20) | 0.4396 | 2.6570 |\n",
    "| Ridge(alpha=100.0)_SelectKBest(k=20) | 0.4395 | 2.6574 |\n",
    "| Ridge(alpha=100.0) | 0.3979 | 2.7509 |\n",
    "| LinearRegression_SelectKBest(k=10) | 0.3760 | 2.8051 |\n",
    "| RandomForestRegressor_SelectKBest(k=20) | 0.3549 | 2.8531 |\n",
    "| LinearRegression_PCA(n=50) | 0.2842 | 3.0068 |\n",
    "| LinearRegression_PCA(n=30) | 0.2777 | 3.0076 |\n",
    "| LinearRegression_PCA(n=20) | 0.2736 | 3.0283 |\n",
    "| RandomForestRegressor | 0.2719 | 3.0314 |\n",
    "| **Baseline Linear Regression** | **0.3750** | **2.8024** |\n",
    "\n",
    "The best model was **Lasso Regression with alpha=0.1**, achieving a cross-validation R² of 0.4861, which represents an improvement of 0.1111 (11.11 percentage points) over the baseline model. Let's optimize this model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90740b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Lasso to select features\n",
    "base_alpha = 0.1\n",
    "\n",
    "# Use Lasso to select features\n",
    "feature_selector = Lasso(alpha=base_alpha, max_iter=10000)\n",
    "feature_selector.fit(X, y_output)\n",
    "\n",
    "# Get non-zero coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': feature_selector.coef_\n",
    "})\n",
    "\n",
    "# Sort by absolute coefficient values\n",
    "coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])\n",
    "coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Filter features with non-zero coefficients\n",
    "non_zero_coef = coef_df[coef_df['Coefficient'] != 0].copy()\n",
    "\n",
    "# Create dataset with selected features only\n",
    "selected_features = non_zero_coef['Feature'].tolist()\n",
    "X_lasso_selected = X[selected_features]\n",
    "\n",
    "print(f\"Original feature count: {X.shape[1]}\")\n",
    "print(f\"Selected feature count: {X_lasso_selected.shape[1]}\")\n",
    "\n",
    "# Display top 10 features by coefficient magnitude\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(non_zero_coef.head(10))\n",
    "\n",
    "# Plot top 20 feature coefficients\n",
    "top_features = non_zero_coef.head(20).sort_values('Coefficient')\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(top_features['Feature'], top_features['Coefficient'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Feature Coefficients')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate base Lasso with selected features\n",
    "lasso_base = Lasso(alpha=base_alpha, max_iter=10000)\n",
    "lasso_results = evaluate_regression_model(lasso_base, X_lasso_selected, y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784be534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of alpha values to test\n",
    "alphas = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3]\n",
    "\n",
    "# Create a dictionary to store results\n",
    "alpha_results = {}\n",
    "\n",
    "# Test each alpha value\n",
    "for alpha in alphas:\n",
    "    lasso_model = Lasso(alpha=alpha, max_iter=10000)\n",
    "    print(f\"\\nTesting Lasso with alpha={alpha}\")\n",
    "    results = evaluate_regression_model(lasso_model, X_lasso_selected, y_output)\n",
    "    alpha_results[alpha] = results['cv_r2']\n",
    "\n",
    "# Find the best alpha\n",
    "best_alpha = max(alpha_results, key=alpha_results.get)\n",
    "best_score = alpha_results[best_alpha]\n",
    "\n",
    "print(f\"\\nBest alpha: {best_alpha} with CV R²: {best_score:.4f}\")\n",
    "\n",
    "# Plot alpha vs R²\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(list(alpha_results.keys()), list(alpha_results.values()), marker='o')\n",
    "plt.axvline(x=best_alpha, color='r', linestyle='--', label=f'Best alpha: {best_alpha}')\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('Cross-Validation R²')\n",
    "plt.title('Lasso Alpha Parameter Tuning')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the best Lasso model\n",
    "best_lasso = Lasso(alpha=best_alpha, max_iter=10000)\n",
    "best_lasso_results = evaluate_regression_model(best_lasso, X_lasso_selected, y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07217424",
   "metadata": {},
   "source": [
    "### Advanced Regression Model Analysis\n",
    "\n",
    "Our optimized Lasso regression model achieved significant improvements over the baseline:\n",
    "\n",
    "1. **Feature Selection**: Lasso automatically identified the most important features from the original 400, reducing dimensionality while improving performance.\n",
    "\n",
    "2. **Regularization**: By penalizing large coefficients, Lasso helped prevent overfitting and improved generalization.\n",
    "\n",
    "3. **Parameter Optimization**: Tuning the alpha parameter further improved performance.\n",
    "\n",
    "4. **Performance Improvement**: The final model achieved a cross-validation R² of approximately 0.50, compared to 0.38 for the baseline model.\n",
    "\n",
    "While there is still room for improvement, the advanced model shows substantially better generalization ability than the baseline linear regression model. I've tried several other techniques to further improve the model, such as adding polynomial features, but it did not give promising results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eea731",
   "metadata": {},
   "source": [
    "## 6. Conclusions and Key Findings\n",
    "\n",
    "### Overall Project Summary\n",
    "\n",
    "In this project, we worked with an artificially generated dataset consisting of 2000 samples and 400 input variables to predict two output variables: a discrete 'Class' variable and a continuous 'Output' variable. Here are the key findings:\n",
    "\n",
    "### Classification Task\n",
    "\n",
    "1. **Baseline Performance**: Logistic Regression using all 400 features achieved a cross-validation accuracy of approximately 52%.\n",
    "\n",
    "2. **Best Model**: Random Forest with Recursive Feature Elimination (RFE) selecting only 16 features achieved a cross-validation accuracy of approximately 80%.\n",
    "\n",
    "3. **Improvement**: An absolute improvement of ~28 percentage points in accuracy.\n",
    "\n",
    "\n",
    "### Regression Task\n",
    "\n",
    "1. **Baseline Performance**: Linear Regression using all 400 features achieved a cross-validation R² of approximately 0.38.\n",
    "\n",
    "2. **Best Model**: Lasso Regression with optimized alpha parameter achieved a cross-validation R² of approximately 0.50.\n",
    "\n",
    "3. **Improvement**: An absolute improvement of ~12 percentage points in R²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7722b0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
